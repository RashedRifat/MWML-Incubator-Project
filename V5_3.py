# -*- coding: utf-8 -*-
"""V5.3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Br4W22PjYB6YYMdXg5vrO_r6ulmfa-V

This project draws heavily from Fast Style Transfer for Arbitrary Styles (establsihed by The TensorFlow Hub Authors, Copyright 2019, Licensed under the Apache License, Version 2.0 (the "License")) and Facebook AI Research's Detectron2 (Licensed under the Apache License, Version 2.0 (the "License")). This liscence can be found in the LISCENCE file.

#Defining Functions
"""

# install dependencies: 
!pip install pyyaml==5.1 pycocotools>=2.0.1
!pip install torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
!gcc --version
  # opencv is pre-installed on colab

  # install detectron2: (colab has CUDA 10.1 + torch 1.5)
  # See https://detectron2.readthedocs.io/tutorials/install.html for instructions
assert torch.__version__.startswith("1.5")
!pip install detectron2==0.2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html

    # Some basic setup:
  # Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

  # import some common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

  # import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg
  #im = cv2.imread("./input.jpg")
  #cv2_imshow(im)

cfg = get_cfg()
  # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
  # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
  #predictor = DefaultPredictor(cfg)
  #outputs = predictor(im)

import functools
import os

from matplotlib import gridspec
import matplotlib.pylab as plt
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

def ask_objects(mask_dict, test=False):
  from random import sample, randrange
  object_list = list(mask_dict.keys())
  #print("Len of object_list (mask_dict keys) = ", len(object_list))
  if len(object_list) == 0:
    print("No objects identified.")
    return list()

  if test:
    return sample(object_list, randrange(0, len(object_list)))

  print("List of objects:", list(mask_dict.keys()))
  valid = True

  inputs = input("Enter a comma seperated sentence of objects to stylize.\nEnter all to select all objects, random for random objects.")
  temp_list = inputs.replace(" ", "").split(",")

  if 'all' in temp_list:
    print("You have selected all keys.")
    return object_list
  if "random" in temp_list:
    print("You have selected random keys.")
    return sample(object_list, randrange(0, len(object_list)))
    
  for x in temp_list:
    x = x.strip()
    if x not in object_list:
      valid = False
  if valid:
    print("Success!")
    return temp_list
  else:
    print("\nEntered objects are not in the object set. Please try again.")
    ans = ask_objects(mask_dict)
    return ans

@functools.lru_cache(maxsize=None)
def load_image(image_path, image_size=(256, 256), preserve_aspect_ratio=True, is_url=False):
  """Loads and preprocesses images."""
  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].
  if is_url:
    image_path = tf.keras.utils.get_file(os.path.basename(image_path)[-128:], image_path)  
  img = plt.imread(image_path).astype(np.float32)[np.newaxis, ...]
  if img.max() > 1.0:
    img = img / 255.
  if len(img.shape) == 3:
    img = tf.stack([img, img, img], axis=-1)
  img = tf.image.resize(img, image_size, preserve_aspect_ratio=True)
  print("Loading Image Success!")
  return img

def show_n(images, titles=('',)):
  n = len(images)
  image_sizes = [image.shape[1] for image in images]
  w = (image_sizes[0] * 6) // 320
  plt.figure(figsize=(w  * n, w))
  gs = gridspec.GridSpec(1, n, width_ratios=image_sizes)
  for i in range(n):
    plt.subplot(gs[i])
    plt.imshow(images[i][0], aspect='equal')
    plt.axis('off')
    plt.title(titles[i] if len(titles) > i else '')
  plt.show()

def ask_content_image(is_demo=False):
  path = input("Please upload an image to stylize to this project directory.\nEnter image path here: ")

  if is_demo:
    path = '/content/input.jpg'
  return path

def run_detectron2(is_demo=False, visualize=True):

  image_path = ask_content_image(is_demo=is_demo)
  im = cv2.imread(image_path)

  cfg = get_cfg()
  # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
  cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
  # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
  cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
  predictor = DefaultPredictor(cfg)
  outputs = predictor(im)

  if visualize:
    print("\n\nShowing Input Image: ")
    cv2_imshow(im)
    print("\n\nShowing Identified Image: ")
    # We can use `Visualizer` to draw the predictions on the image.
    v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1])

  #from detectron2.data import MetadataCatalog
  MetadataCatalog.list()
  class_list = MetadataCatalog.get("coco_2017_train").thing_classes


  return image_path, outputs, class_list

print("As this is a demo, feel free to enter anything.")
result = run_detectron2(is_demo=True)
content_image = result[0]

def create_mask_dict(result):
  outputs = result[1]
  class_list = result[2]
  
  #class_list = MetadataCatalog.get("coco_2014_train").thing_classes
  object_list = outputs["instances"].pred_classes
  mask_list = outputs["instances"].pred_masks
  unique_objects = list()
  object_counter = 0
  for object_ in object_list:
    unique_objects.append(str(class_list[object_]) + str(object_counter))
    object_counter += 1 


  mask_counter = 0
  mask_dict = dict()
  for unique in unique_objects:
    mask_dict[unique] = mask_list[mask_counter].int()
    mask_counter += 1
  
  return mask_dict

mask_dict = create_mask_dict(result)

#Create Style Dict for Urls and another for user-input styles. 
style_dict_urls = dict(
  kanagawa_great_wave='https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg',
  kandinsky_composition_7='https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg',
  hubble_pillars_of_creation='https://upload.wikimedia.org/wikipedia/commons/6/68/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg',
  van_gogh_starry_night='https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg',
  turner_nantes='https://upload.wikimedia.org/wikipedia/commons/b/b7/JMW_Turner_-_Nantes_from_the_Ile_Feydeau.jpg',
  munch_scream='https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg',
  picasso_demoiselles_avignon='https://upload.wikimedia.org/wikipedia/en/4/4c/Les_Demoiselles_d%27Avignon.jpg',
  picasso_violin='https://upload.wikimedia.org/wikipedia/en/3/3c/Pablo_Picasso%2C_1911-12%2C_Violon_%28Violin%29%2C_oil_on_canvas%2C_Kr%C3%B6ller-M%C3%BCller_Museum%2C_Otterlo%2C_Netherlands.jpg',
  picasso_bottle_of_rum='https://upload.wikimedia.org/wikipedia/en/7/7f/Pablo_Picasso%2C_1911%2C_Still_Life_with_a_Bottle_of_Rum%2C_oil_on_canvas%2C_61.3_x_50.5_cm%2C_Metropolitan_Museum_of_Art%2C_New_York.jpg',
  fire='https://upload.wikimedia.org/wikipedia/commons/3/36/Large_bonfire.jpg',
  derkovits_woman_head='https://upload.wikimedia.org/wikipedia/commons/0/0d/Derkovits_Gyula_Woman_head_1922.jpg',
  amadeo_style_life='https://upload.wikimedia.org/wikipedia/commons/8/8e/Untitled_%28Still_life%29_%281913%29_-_Amadeo_Souza-Cardoso_%281887-1918%29_%2817385824283%29.jpg',
  derkovtis_talig='https://upload.wikimedia.org/wikipedia/commons/3/37/Derkovits_Gyula_Talig%C3%A1s_1920.jpg',
  amadeo_cardoso='https://upload.wikimedia.org/wikipedia/commons/7/7d/Amadeo_de_Souza-Cardoso%2C_1915_-_Landscape_with_black_figure.jpg'
)

def ask_user_styles():
  user_styles = dict()
  inp = input("Do you wish to upload a style image? Y/N ")
  if inp.lower() == "y" or inp.lower() == 'yes':
    image_name = str(input("Paste image name here:")).strip()
    image_path = input("Paste image path here:").strip()
    user_styles[image_name] = image_path
  elif inp.lower() == "n" or inp.lower() == 'no':
    print("No image loaded.\n")
  else:
    print("Unable to determine answer. Please try again.")
    ask_user_styles()
  return user_styles

def ask_multiple_styles(style_dict_urls, user_styles):
  print("Please select a style from the list below.\nIf you select multiple styles, you must select objects for each style.")
  print("Enter multiple styles using a comma seperated sentence.\n")
  style_names = list(style_dict_urls.keys()) + list(user_styles.keys())
  print(style_names)
  selected_styles = input("\nEnter Styles:")
  selected_styles = selected_styles.lower().replace(" ", "").split(",")
  ans = []
  err = []
  for ip in selected_styles:
    if ip not in style_names:
      err.append(ip)
    else:
      ans.append(ip)
  if len(err) > 0:
    print("The follow styles faid to load from ask_multiple_styles. ", err, "\n")
  if len(ans) == 0:
    print("No styles selected. Please try again.\n")
    ans = ask_multiple_styles(style_dict_urls, user_styles)
    return ans
  else:
    return ans

#Update style_dict with user styles. 
def load_style_dict(slected_styles, style_dict_urls, user_styles):
  style_dict = dict()
  err = "The following styles failed to load from load_style_dict. Please spell them correctly or load them if inputting user styles.\n"
  valid = len(err)
  if len(selected_styles) == 0:
    print("No styles to load.\n")
    return
  for style in selected_styles:
    if style in list(style_dict_urls.keys()):
      style_dict[style] = load_image(style_dict_urls[style], is_url=True)
      style_dict[style] = tf.nn.avg_pool(style_dict[style], ksize=[3,3], strides=[1,1], padding='SAME')
    elif style in list(user_styles.keys()):
      style_dict[style] = load_image(user_styles[style])
      style_dict[style] = tf.nn.avg_pool(style_dict[style], ksize=[3,3], strides=[1,1], padding='SAME')
    else:
      err += str(style) + "\n"
  if len(err) > valid:
    print(err)
  return style_dict

user_styles = ask_user_styles()

selected_styles = ask_multiple_styles(style_dict_urls, user_styles)
print("\n\nSelected_styles is: ", selected_styles)

style_dict = load_style_dict(selected_styles, style_dict_urls, user_styles)

print(user_styles)
print(selected_styles)
print(style_dict.keys())


def ask_per_style(style_dict, mask_dict):
  #object_list = list(mask_dict.keys())
  #object_masks = list()
  mask_dict_copy = mask_dict.copy()
  styles = list(style_dict.keys())
  per_style_objects = dict()
  per_style_mask = dict()

  #Get objects to be stylized per style 
  for style in styles:
    print("\nFor style: ", style, " select the list of objects to be stylized.\n")
    value = ask_objects(mask_dict_copy)
    if value == None:
      print("No objects selected.")
    else:
      for x in value:
        mask_dict_copy.pop(x)
    per_style_objects[style] = value 
  return styles, per_style_objects

output = ask_per_style(style_dict, mask_dict)
print("\n", output[1])

test = list(output[1].values())
print(test)
print(len(test[0]))

def get_masks_per_style(per_style_objects, mask_dict):
    #Returns an element sorted list where each value corresponds to 
    #the tensor mask of the object defined in per_style_objects. 
    #Essientally stores the masks of each object as shown in the test function above. 

  object_list = list(per_style_objects.values())
  per_style_mask = list()
  per_style_inverse = list()

  #Get masks for each object
  for list_ in object_list:
    per_style_mask.append(list())
    per_style_inverse.append(list())

    if len(list_) == 0:
      continue 
    else:
      for obj in list_:
        mask = mask_dict[obj]
        per_style_mask[-1].append(mask)   
  return per_style_mask

per_style_mask = get_masks_per_style(output[1], mask_dict)
#print(len(per_style_mask[0]))

class Style_obj():
  def __init__(self):
    self.content_image = ""
    self.style_names = list()
    self.styles = list()
    self.objects = list()
    self.masks = list()
    self.inverse_mask = list()
    self.true_masks = list()
    self.stylized_objects = list()

  def display_in_conjunction(self, index=0):
    #Reminder that only style is a one-list, rest are nested lists!
    print("Style Name is: ", str(self.style_names[index]))
    print("Associated Loaded Style Image is: ", len(self.styles[index]))
    print("Associated Objects are: ", self.objects[index])
    print("Assocated Masks are of length: ", len(self.masks[index]))
    print("Assocated Inverse Mask is of shape: ", self.inverse_mask.shape)

  def load_inverse_masks(self):
    #Validate and create mask_shape
    for x in self.masks:
      if len(x) == 0:
        continue 
      else:
        self.mask_shape = x[0].shape
        break 

    #Create inverse-mask
    inv_mask = torch.ones(self.mask_shape).numpy()
    for lis in self.masks:
      if len(lis) == 0:
        continue 
      else:
        for obj in lis:
          obj = obj.cpu().numpy()
          inv_mask = np.where(inv_mask != obj, inv_mask, inv_mask - obj)
          #Realzie the shape of the masks are [480,640], which requires special broadcasting with tensors of shape [480,640,3]
    
    self.inverse_mask = inv_mask

  def combine_masks(self):
    if len(self.masks) == 0:
      return "No masks detected."
    for index in range(0, len(self.masks)):
      #list_len = len(self.masks[index])
      base = torch.zeros(self.mask_shape).numpy()
      for indiv in self.masks[index]:
        base = np.maximum(base, indiv.cpu().numpy())
      self.true_masks.append(base)
    print("True_Mask Size: ", len(self.true_masks))
    print("Inv_Mask Size: ", self.inverse_mask.shape)
  
  def pass_to_nst(self, save=True):
    # Load TF-Hub module.
    hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'
    hub_module = hub.load(hub_handle)
    #This loaded_content function requires using the shape of the masl 
    loaded_content = load_image(self.content_image, image_size=(self.mask_shape))
    loaded_style = self.styles
    counter = 0
    for instance in loaded_style:
      outputs = hub_module(tf.constant(loaded_content), tf.constant(instance))
      stylized_image = outputs[0]
      filename = "partial_stylized_image" + str(counter) + ".jpg"
      tf.keras.preprocessing.image.save_img(filename, stylized_image[0])
      stylized_objects = cv2.imread(filename)
      #cv2_imshow(stylized_objects)

    #Get Stylized Objects  
      mask = self.true_masks[counter]
      counter += 1
      for i in range(0, stylized_objects.shape[2]):
        stylized_objects[:,:,i] = stylized_objects[:,:,i] * mask[:,:] 
      #cv2_imshow(stylized_objects)
      self.stylized_objects.append(stylized_objects)
    print("Stylized Objects: ", len(self.stylized_objects))
    print("Stylized Objects Shape: ", self.stylized_objects[0].shape)

    #Get inverse mask 
    inv_mask = self.inverse_mask
    base_image = cv2.imread(content_image)
    for i in range(0, base_image.shape[2]):
      base_image[:,:,i] = base_image[:,:,i] * inv_mask[:,:]
    print("\n\nBase_image ", base_image.shape)
    #cv2_imshow(base_image)

    #Combine all!
    for image in self.stylized_objects:
      for i in range(0, 3):
          base_image[:,:,i] = np.maximum(base_image[:,:,i], image[:,:,i])
    cv2_imshow(base_image)

    if save:
      outpath = 'final_image.jpg'
      cv2.imwrite(outpath, base_image)
      print("\n\n\nImage Saved! Filename is ", outpath)
    
  def testing_function(self):
    filename = "input.jpg"
    image = cv2.imread(filename)
    for i in range(0,3):
      image[:,:,i] = image[:,:,i] * self.true_masks[0][:,:]
    cv2_imshow(image)

repo = Style_obj()
repo.content_image = content_image
repo.style_names = selected_styles
repo.styles = list(style_dict.values())
repo.objects = list(output[1].values())
repo.masks = per_style_mask
repo.load_inverse_masks()
repo.combine_masks()

repo.display_in_conjunction(0)

#print(repo.content_image)

repo.pass_to_nst()

